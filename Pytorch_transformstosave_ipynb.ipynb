{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yuanzihui0810/pytorch_exercise/blob/main/Pytorch_transformstosave_ipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1xT9ry2XMwe"
      },
      "source": [
        "# Transforms\n",
        "\n",
        "Data does not always come in its final processed form that is required\n",
        "for training machine learning algorithms. We use **transforms** to\n",
        "perform some manipulation of the data and make it suitable for training.\n",
        "\n",
        "All TorchVision datasets have two parameters -**`transform` to modify the\n",
        "features** and **`target_transform` to modify the labels** - that accept\n",
        "callables containing the transformation logic. The\n",
        "[torchvision.transforms](https://pytorch.org/vision/stable/transforms.html)\n",
        "module offers several commonly-used transforms out of the box.\n",
        "\n",
        "The FashionMNIST features are in PIL Image format, and the labels are\n",
        "integers. For training, we need the features as normalized tensors, and\n",
        "the labels as **one-hot** encoded tensors. To make these transformations, we\n",
        "use `ToTensor` and `Lambda`.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch #cs\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor, Lambda"
      ],
      "metadata": {
        "id": "H0bcA_SsY-tM"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bYbmkLLBXMwf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee0a6e70-126e-4e5b-ebb4-a6fa8dd5ca96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26.4M/26.4M [00:02<00:00, 11.2MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29.5k/29.5k [00:00<00:00, 200kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.42M/4.42M [00:01<00:00, 3.74MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5.15k/5.15k [00:00<00:00, 4.06MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "ds = datasets.FashionMNIST(\n",
        "    root=\"data\", # 下载路径\n",
        "    train=True, # 加载训练集 False是训练集 默认加载训练集\n",
        "    download=True, # 如果该数据集尚未存在于指定的路径 则会下载\n",
        "    transform=ToTensor(), # PIL图像格式 转换成 PyTorch张量Tensor\n",
        "    target_transform=Lambda(lambda y: torch.zeros(10, dtype=torch.float).scatter_(0, torch.tensor(y), value=1))\n",
        "    # transform = Lambda(lambda x: <自定义操作>) 进行自定义操作\n",
        "    # lambda建立标签scatter_(0, torch.tensor(y), value=1)转换为onehot编码\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 数据集内容\n",
        "ds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_924qPewaebb",
        "outputId": "06de4ad1-11b9-416c-9e88-888cf0146cb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset FashionMNIST\n",
              "    Number of datapoints: 60000\n",
              "    Root location: data\n",
              "    Split: Train\n",
              "    StandardTransform\n",
              "Transform: ToTensor()\n",
              "Target transform: Lambda()"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 数据内容\n",
        "ds[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dxBC6nVbaqjd",
        "outputId": "27bfadeb-112a-4007-fb98-7240baa8e99b",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.0000, 0.0510,\n",
              "           0.2863, 0.0000, 0.0000, 0.0039, 0.0157, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0039, 0.0039, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0118, 0.0000, 0.1412, 0.5333,\n",
              "           0.4980, 0.2431, 0.2118, 0.0000, 0.0000, 0.0000, 0.0039, 0.0118,\n",
              "           0.0157, 0.0000, 0.0000, 0.0118],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0235, 0.0000, 0.4000, 0.8000,\n",
              "           0.6902, 0.5255, 0.5647, 0.4824, 0.0902, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0471, 0.0392, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6078, 0.9255,\n",
              "           0.8118, 0.6980, 0.4196, 0.6118, 0.6314, 0.4275, 0.2510, 0.0902,\n",
              "           0.3020, 0.5098, 0.2824, 0.0588],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.2706, 0.8118, 0.8745,\n",
              "           0.8549, 0.8471, 0.8471, 0.6392, 0.4980, 0.4745, 0.4784, 0.5725,\n",
              "           0.5529, 0.3451, 0.6745, 0.2588],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0039, 0.0039, 0.0039, 0.0000, 0.7843, 0.9098, 0.9098,\n",
              "           0.9137, 0.8980, 0.8745, 0.8745, 0.8431, 0.8353, 0.6431, 0.4980,\n",
              "           0.4824, 0.7686, 0.8980, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7176, 0.8824, 0.8471,\n",
              "           0.8745, 0.8941, 0.9216, 0.8902, 0.8784, 0.8706, 0.8784, 0.8667,\n",
              "           0.8745, 0.9608, 0.6784, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7569, 0.8941, 0.8549,\n",
              "           0.8353, 0.7765, 0.7059, 0.8314, 0.8235, 0.8275, 0.8353, 0.8745,\n",
              "           0.8627, 0.9529, 0.7922, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0039, 0.0118, 0.0000, 0.0471, 0.8588, 0.8627, 0.8314,\n",
              "           0.8549, 0.7529, 0.6627, 0.8902, 0.8157, 0.8549, 0.8784, 0.8314,\n",
              "           0.8863, 0.7725, 0.8196, 0.2039],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0235, 0.0000, 0.3882, 0.9569, 0.8706, 0.8627,\n",
              "           0.8549, 0.7961, 0.7765, 0.8667, 0.8431, 0.8353, 0.8706, 0.8627,\n",
              "           0.9608, 0.4667, 0.6549, 0.2196],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0157, 0.0000, 0.0000, 0.2157, 0.9255, 0.8941, 0.9020,\n",
              "           0.8941, 0.9412, 0.9098, 0.8353, 0.8549, 0.8745, 0.9176, 0.8510,\n",
              "           0.8510, 0.8196, 0.3608, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0039, 0.0157, 0.0235, 0.0275, 0.0078, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.9294, 0.8863, 0.8510, 0.8745,\n",
              "           0.8706, 0.8588, 0.8706, 0.8667, 0.8471, 0.8745, 0.8980, 0.8431,\n",
              "           0.8549, 1.0000, 0.3020, 0.0000],\n",
              "          [0.0000, 0.0118, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.2431, 0.5686, 0.8000, 0.8941, 0.8118, 0.8353, 0.8667,\n",
              "           0.8549, 0.8157, 0.8275, 0.8549, 0.8784, 0.8745, 0.8588, 0.8431,\n",
              "           0.8784, 0.9569, 0.6235, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0706, 0.1725, 0.3216, 0.4196,\n",
              "           0.7412, 0.8941, 0.8627, 0.8706, 0.8510, 0.8863, 0.7843, 0.8039,\n",
              "           0.8275, 0.9020, 0.8784, 0.9176, 0.6902, 0.7373, 0.9804, 0.9725,\n",
              "           0.9137, 0.9333, 0.8431, 0.0000],\n",
              "          [0.0000, 0.2235, 0.7333, 0.8157, 0.8784, 0.8667, 0.8784, 0.8157,\n",
              "           0.8000, 0.8392, 0.8157, 0.8196, 0.7843, 0.6235, 0.9608, 0.7569,\n",
              "           0.8078, 0.8745, 1.0000, 1.0000, 0.8667, 0.9176, 0.8667, 0.8275,\n",
              "           0.8627, 0.9098, 0.9647, 0.0000],\n",
              "          [0.0118, 0.7922, 0.8941, 0.8784, 0.8667, 0.8275, 0.8275, 0.8392,\n",
              "           0.8039, 0.8039, 0.8039, 0.8627, 0.9412, 0.3137, 0.5882, 1.0000,\n",
              "           0.8980, 0.8667, 0.7373, 0.6039, 0.7490, 0.8235, 0.8000, 0.8196,\n",
              "           0.8706, 0.8941, 0.8824, 0.0000],\n",
              "          [0.3843, 0.9137, 0.7765, 0.8235, 0.8706, 0.8980, 0.8980, 0.9176,\n",
              "           0.9765, 0.8627, 0.7608, 0.8431, 0.8510, 0.9451, 0.2549, 0.2863,\n",
              "           0.4157, 0.4588, 0.6588, 0.8588, 0.8667, 0.8431, 0.8510, 0.8745,\n",
              "           0.8745, 0.8784, 0.8980, 0.1137],\n",
              "          [0.2941, 0.8000, 0.8314, 0.8000, 0.7569, 0.8039, 0.8275, 0.8824,\n",
              "           0.8471, 0.7255, 0.7725, 0.8078, 0.7765, 0.8353, 0.9412, 0.7647,\n",
              "           0.8902, 0.9608, 0.9373, 0.8745, 0.8549, 0.8314, 0.8196, 0.8706,\n",
              "           0.8627, 0.8667, 0.9020, 0.2627],\n",
              "          [0.1882, 0.7961, 0.7176, 0.7608, 0.8353, 0.7725, 0.7255, 0.7451,\n",
              "           0.7608, 0.7529, 0.7922, 0.8392, 0.8588, 0.8667, 0.8627, 0.9255,\n",
              "           0.8824, 0.8471, 0.7804, 0.8078, 0.7294, 0.7098, 0.6941, 0.6745,\n",
              "           0.7098, 0.8039, 0.8078, 0.4510],\n",
              "          [0.0000, 0.4784, 0.8588, 0.7569, 0.7020, 0.6706, 0.7176, 0.7686,\n",
              "           0.8000, 0.8235, 0.8353, 0.8118, 0.8275, 0.8235, 0.7843, 0.7686,\n",
              "           0.7608, 0.7490, 0.7647, 0.7490, 0.7765, 0.7529, 0.6902, 0.6118,\n",
              "           0.6549, 0.6941, 0.8235, 0.3608],\n",
              "          [0.0000, 0.0000, 0.2902, 0.7412, 0.8314, 0.7490, 0.6863, 0.6745,\n",
              "           0.6863, 0.7098, 0.7255, 0.7373, 0.7412, 0.7373, 0.7569, 0.7765,\n",
              "           0.8000, 0.8196, 0.8235, 0.8235, 0.8275, 0.7373, 0.7373, 0.7608,\n",
              "           0.7529, 0.8471, 0.6667, 0.0000],\n",
              "          [0.0078, 0.0000, 0.0000, 0.0000, 0.2588, 0.7843, 0.8706, 0.9294,\n",
              "           0.9373, 0.9490, 0.9647, 0.9529, 0.9569, 0.8667, 0.8627, 0.7569,\n",
              "           0.7490, 0.7020, 0.7137, 0.7137, 0.7098, 0.6902, 0.6510, 0.6588,\n",
              "           0.3882, 0.2275, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1569,\n",
              "           0.2392, 0.1725, 0.2824, 0.1608, 0.1373, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000]]]),\n",
              " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]))"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ds[0][0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tcx8S3aDbDHx",
        "outputId": "73455210-ae0b-4cb7-8254-f133d03aed22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 28, 28])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.imshow(ds[0][0].squeeze(), cmap=\"gray\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "wIZslNDrp-Od",
        "outputId": "87900145-38c5-4cc5-cfd6-f4eb661f5edc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f9a842790f0>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMSbQ_qXXMwf"
      },
      "source": [
        "## ToTensor()\n",
        "\n",
        "[ToTensor](https://pytorch.org/vision/stable/transforms.html#torchvision.transforms.ToTensor)\n",
        "converts a PIL image or NumPy `ndarray` into a `FloatTensor`. and scales\n",
        "the image\\'s pixel intensity values in the range \\[0., 1.\\]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "transform = ToTensor()\n",
        "random_array = np.random.rand(28,28,1) #(height, width, channels)\n",
        "transform(random_array).shape #(channels, height, width)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mnGlBa28dRzO",
        "outputId": "a847678e-da0f-409e-d66b-9c4a535e052b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 28, 28])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9niVHkz7XMwf"
      },
      "source": [
        "## Lambda Transforms\n",
        "\n",
        "Lambda transforms apply any user-defined lambda function. Here, we\n",
        "define a function to turn the integer into a one-hot encoded tensor. It\n",
        "first creates a zero tensor of size 10 (the number of labels in our\n",
        "dataset) and calls\n",
        "[scatter\\_](https://pytorch.org/docs/stable/generated/torch.Tensor.scatter_.html)\n",
        "which assigns a `value=1` on the index as given by the label `y`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eLj6k2-bXMwg"
      },
      "outputs": [],
      "source": [
        "target_transform = Lambda(lambda y: torch.zeros(\n",
        "    10, dtype=torch.float).scatter_(dim=0, index=torch.tensor(y), value=1))\n",
        "# scatter_原地操作 dim指定操作的维度 index指定值分配的索引位置 value赋的值\n",
        "# or target_transform = Lambda(lambda y: torch.eye(10)[y])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Augmentation\n",
        "[Transforming and augmenting images](https://pytorch.org/vision/stable/transforms.html)\n",
        "\n",
        "常用数据增强方法：\\\n",
        "RandomVerticalFlip()：随机垂直翻转。\\\n",
        "RandomResizedCrop()：随机裁剪并调整图像大小。\\\n",
        "RandomAffine()：应用随机仿射变换（如旋转、平移、缩放、剪切等）。\\\n",
        "GaussianBlur()：随机对图像应用高斯模糊。\\\n",
        "RandomErasing()：在图像上随机擦除部分区域。"
      ],
      "metadata": {
        "id": "OlvwJv2ZpGI2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Image Classification\n",
        "import torch\n",
        "from torchvision.transforms import transforms\n",
        "\n",
        "img = ds[0][0]\n",
        "\n",
        "transforms1 = transforms.Compose([\n",
        "    transforms.RandomRotation(30), # 随机旋转图片，范围是 [-30, 30] 度\n",
        "    transforms.RandomHorizontalFlip(), # 随机水平翻转图片\n",
        "    # ToTensor(),\n",
        "])\n",
        "\n",
        "img = transforms1(img)"
      ],
      "metadata": {
        "id": "I9Sh3Sp5q4Rz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(img.squeeze(), cmap=\"gray\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "KOmVJlzar2-6",
        "outputId": "42c08bc3-6d92-4461-cf9e-5c50847cf02c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f9a821ab550>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YyjwAsnnXMwg"
      },
      "source": [
        "\n",
        "------------------------------------------------------------------------\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVHIB1wneyjP"
      },
      "source": [
        "# Build the Neural Network\n",
        "\n",
        "Neural networks comprise of layers/modules that perform operations on\n",
        "data. The [torch.nn](https://pytorch.org/docs/stable/nn.html) namespace\n",
        "provides all the building blocks you need to build your own neural\n",
        "network. Every module in PyTorch subclasses the\n",
        "[nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html).\n",
        "A neural network is a module itself that consists of other modules\n",
        "(layers). This nested structure allows for building and managing complex\n",
        "architectures easily.\n",
        "\n",
        "In the following sections, we\\'ll build a neural network to classify\n",
        "images in the FashionMNIST dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rl59ZEIpeyjQ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZB5TXiKeyjQ"
      },
      "source": [
        "## Get Device for Training\n",
        "\n",
        "We want to be able to train our model on a hardware accelerator like the\n",
        "GPU or MPS, if available. Let\\'s check to see if\n",
        "[torch.cuda](https://pytorch.org/docs/stable/notes/cuda.html) or\n",
        "[torch.backends.mps](https://pytorch.org/docs/stable/notes/mps.html) are\n",
        "available, otherwise we use the CPU.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SI7YTUJZeyjR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb65526c-51f9-4cdd-e503-a951d461e963"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n"
          ]
        }
      ],
      "source": [
        "device = (\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"mps\"\n",
        "    if torch.backends.mps.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "print(f\"Using {device} device\") # 确定是gpu跑"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sAz1ll6ceyjR"
      },
      "source": [
        "## Define the Class\n",
        "\n",
        "We define our neural network by subclassing `nn.Module`, and initialize\n",
        "the neural network layers in `__init__`. Every `nn.Module` subclass\n",
        "implements the operations on input data in the `forward` method.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N2BO0Ni0eyjR"
      },
      "outputs": [],
      "source": [
        "class NeuralNetwork(nn.Module): # 继承nn.Module父类 实现init forward两个主要方法\n",
        "    \"\"\"\n",
        "    __init__ 方法:\n",
        "    是模型的构造函数，用于初始化网络的各个层。\n",
        "    层：\n",
        "    基础层：\n",
        "    全连接层：nn.Linear(in_features, out_features)\n",
        "    卷积层：nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
        "    池化层：nn.MaxPool2d(kernel_size, stride, padding)\n",
        "    正则化层：\n",
        "    批归一化层：nn.BatchNorm2d(num_features)\n",
        "    Dropout层：nn.Dropout(p) p是丢弃神经元概率\n",
        "    递归层：\n",
        "    以及nn.LSTM, nn.GRU\n",
        "\n",
        "    激活函数：\n",
        "    nn.ReLU()\n",
        "    nn.Sigmoid()\n",
        "    nn.Tanh()\n",
        "    nn.LeakyReLU()\n",
        "    nn.Softmax(dim) 多分类\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__() # 继承nn.Module中的所有功能\n",
        "        self.flatten = nn.Flatten() # 将输入的28x28图像展平为一个一维向量\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(28*28, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 10),\n",
        "            # nn.Softmax(1)\n",
        "        ) # 这里用了nn.Sequential 可以将多个层按顺序组合\n",
        "\n",
        "    def forward(self, x): # 前向传播 应用\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 逐层定义\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(28*28, 512)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(512, 512)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.fc3 = nn.Linear(512, 10)\n",
        "        # self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.relu2(x)\n",
        "        logits = self.fc3(x)\n",
        "        # probs = self.softmax(logits)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "FPGL3fcMeuIl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJ8PJBcreyjR"
      },
      "source": [
        "We create an instance of `NeuralNetwork`, and move it to the `device`,\n",
        "and print its structure.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "30DtFqLkeyjR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3ca1ff1-11c6-4b8e-e920-6272674a1441"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NeuralNetwork(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (fc1): Linear(in_features=784, out_features=512, bias=True)\n",
            "  (relu1): ReLU()\n",
            "  (fc2): Linear(in_features=512, out_features=512, bias=True)\n",
            "  (relu2): ReLU()\n",
            "  (fc3): Linear(in_features=512, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "model = NeuralNetwork().to(device)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIPG6AwheyjR"
      },
      "source": [
        "To use the model, we pass it the input data. This executes the model\\'s\n",
        "`forward`, along with some [background\n",
        "operations](https://github.com/pytorch/pytorch/blob/270111b7b611d174967ed204776985cefca9c144/torch/nn/modules/module.py#L866).\n",
        "Do not call `model.forward()` directly!\n",
        "\n",
        "Calling the model on the input returns a 2-dimensional tensor with dim=0\n",
        "corresponding to each output of 10 raw predicted values for each class,\n",
        "and dim=1 corresponding to the individual values of each output. We get\n",
        "the prediction probabilities by passing it through an instance of the\n",
        "`nn.Softmax` module.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mn11BMjkeyjS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbe1669c-5d2c-4b94-9f98-40eaffe43e02"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "logits: tensor([[-0.0089,  0.0311, -0.0209, -0.0732, -0.0453,  0.0602, -0.0100,  0.0629,\n",
            "          0.0722,  0.1066]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "pred_probab: tensor([[0.0972, 0.1012, 0.0961, 0.0912, 0.0938, 0.1042, 0.0971, 0.1045, 0.1055,\n",
            "         0.1092]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
            "Predicted class: tensor([9], device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "X = torch.rand(1, 28, 28, device=device)\n",
        "logits = model(X)\n",
        "print(f\"logits: {logits}\")\n",
        "\n",
        "pred_probab = nn.Softmax(dim=1)(logits)\n",
        "print(f\"pred_probab: {pred_probab}\")\n",
        "\n",
        "y_pred = pred_probab.argmax(1)\n",
        "print(f\"Predicted class: {y_pred}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DbFGt68BeyjS"
      },
      "source": [
        "------------------------------------------------------------------------\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "isoUlVIFeyjS"
      },
      "source": [
        "## Model Layers\n",
        "\n",
        "Let\\'s break down the layers in the FashionMNIST model. To illustrate\n",
        "it, we will take a sample minibatch of 3 images of size 28x28 and see\n",
        "what happens to it as we pass it through the network.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M3Z3K5y3eyjS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6924bd3d-ba5c-45dd-fe81-959ba8bea00f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 1, 28, 28])\n"
          ]
        }
      ],
      "source": [
        "input_image = torch.rand(3,1,28,28)\n",
        "print(input_image.size())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AdT2O217eyjS"
      },
      "source": [
        "### nn.Flatten\n",
        "\n",
        "We initialize the\n",
        "[nn.Flatten](https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html)\n",
        "layer to convert each 2D 28x28 image into a contiguous array of 784\n",
        "pixel values ( the minibatch dimension (at dim=0) is maintained).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H-glFaqReyjS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f56bd4bb-487d-40b5-8cce-75c16fef93ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 784])\n"
          ]
        }
      ],
      "source": [
        "flatten = nn.Flatten()\n",
        "flat_image = flatten(input_image)\n",
        "print(flat_image.size())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWMsV49LeyjS"
      },
      "source": [
        "### nn.Linear\n",
        "\n",
        "The [linear\n",
        "layer](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html)\n",
        "is a module that applies a linear transformation on the input using its\n",
        "stored weights and biases.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rCHI7VyReyjS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ad884eb-9e73-44cb-de66-0735c4930bc1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 20])\n"
          ]
        }
      ],
      "source": [
        "layer1 = nn.Linear(in_features=28*28, out_features=20)\n",
        "hidden1 = layer1(flat_image)\n",
        "print(hidden1.size())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZRn9SbueyjS"
      },
      "source": [
        "### nn.ReLU\n",
        "\n",
        "Non-linear activations are what create the complex mappings between the\n",
        "model\\'s inputs and outputs. They are applied after linear\n",
        "transformations to introduce *nonlinearity*, helping neural networks\n",
        "learn a wide variety of phenomena.\n",
        "\n",
        "In this model, we use\n",
        "[nn.ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html)\n",
        "between our linear layers, but there\\'s other activations to introduce\n",
        "non-linearity in your model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7_lSiyxheyjS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e85330bb-65ee-4347-d595-159c8e103d88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before ReLU: tensor([[ 0.3150,  0.0366,  0.3891,  0.1197, -0.0540, -0.5567, -0.0742, -0.1864,\n",
            "          0.1171, -0.0140,  0.3528,  0.0904, -0.3077,  0.1754, -0.3865,  0.0214,\n",
            "          0.5198,  0.3332, -0.1507,  0.2487],\n",
            "        [ 0.2977,  0.4671,  0.2803, -0.0269,  0.3137, -0.3235,  0.4250, -0.2669,\n",
            "         -0.0405,  0.0422,  0.5966,  0.3578,  0.0543,  0.0685, -0.3848, -0.0638,\n",
            "          0.3745,  0.5006, -0.2912, -0.0322],\n",
            "        [ 0.3518,  0.3402,  0.3476,  0.0563,  0.3652, -0.5894,  0.1897,  0.0168,\n",
            "          0.0221, -0.0169,  0.1602,  0.3581, -0.2244,  0.2580, -0.6708,  0.1210,\n",
            "          0.4275,  0.4231,  0.0044, -0.0019]], grad_fn=<AddmmBackward0>)\n",
            "\n",
            "\n",
            "After ReLU: tensor([[0.3150, 0.0366, 0.3891, 0.1197, 0.0000, 0.0000, 0.0000, 0.0000, 0.1171,\n",
            "         0.0000, 0.3528, 0.0904, 0.0000, 0.1754, 0.0000, 0.0214, 0.5198, 0.3332,\n",
            "         0.0000, 0.2487],\n",
            "        [0.2977, 0.4671, 0.2803, 0.0000, 0.3137, 0.0000, 0.4250, 0.0000, 0.0000,\n",
            "         0.0422, 0.5966, 0.3578, 0.0543, 0.0685, 0.0000, 0.0000, 0.3745, 0.5006,\n",
            "         0.0000, 0.0000],\n",
            "        [0.3518, 0.3402, 0.3476, 0.0563, 0.3652, 0.0000, 0.1897, 0.0168, 0.0221,\n",
            "         0.0000, 0.1602, 0.3581, 0.0000, 0.2580, 0.0000, 0.1210, 0.4275, 0.4231,\n",
            "         0.0044, 0.0000]], grad_fn=<ReluBackward0>)\n"
          ]
        }
      ],
      "source": [
        "print(f\"Before ReLU: {hidden1}\\n\\n\")\n",
        "hidden1 = nn.ReLU()(hidden1)\n",
        "print(f\"After ReLU: {hidden1}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6GHLAwQeyjS"
      },
      "source": [
        "### nn.Sequential\n",
        "\n",
        "[nn.Sequential](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html)\n",
        "is an ordered container of modules. The data is passed through all the\n",
        "modules in the same order as defined. You can use sequential containers\n",
        "to put together a quick network like `seq_modules`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lyXMSl8QeyjS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9331987-0d61-458b-a680-813de298c3b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "logits: tensor([[-0.0703,  0.0237, -0.0885, -0.0026, -0.2140, -0.1440,  0.1006,  0.0778,\n",
            "          0.0777,  0.1776],\n",
            "        [-0.0546,  0.0092, -0.2233, -0.0450, -0.1235, -0.2716,  0.1285,  0.0916,\n",
            "          0.1684,  0.2524],\n",
            "        [-0.1561,  0.0042, -0.2865, -0.2390, -0.1653, -0.1451,  0.0744,  0.0293,\n",
            "          0.0780,  0.0985]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ],
      "source": [
        "seq_modules = nn.Sequential(\n",
        "    flatten,\n",
        "    layer1,\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(20, 10)\n",
        ")\n",
        "input_image = torch.rand(3,28,28)\n",
        "logits = seq_modules(input_image)\n",
        "\n",
        "print(f\"logits: {logits}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9UTLa69eyjT"
      },
      "source": [
        "### nn.Softmax\n",
        "\n",
        "The last linear layer of the neural network returns [logits]{.title-ref}\n",
        "- raw values in \\[-infty, infty\\] - which are passed to the\n",
        "[nn.Softmax](https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html)\n",
        "module. The logits are scaled to values \\[0, 1\\] representing the\n",
        "model\\'s predicted probabilities for each class. `dim` parameter\n",
        "indicates the dimension along which the values must sum to 1.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "adlsgdXZeyjT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b858bb79-f290-412d-d811-284eeb58eef2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pred_probab: tensor([[0.0932, 0.1024, 0.0915, 0.0997, 0.0807, 0.0866, 0.1105, 0.1081, 0.1080,\n",
            "         0.1194],\n",
            "        [0.0941, 0.1003, 0.0795, 0.0950, 0.0878, 0.0758, 0.1130, 0.1089, 0.1176,\n",
            "         0.1279],\n",
            "        [0.0910, 0.1068, 0.0799, 0.0837, 0.0902, 0.0920, 0.1146, 0.1095, 0.1150,\n",
            "         0.1174]], grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ],
      "source": [
        "softmax = nn.Softmax(dim=1)\n",
        "pred_probab = softmax(logits)\n",
        "\n",
        "print(f\"pred_probab: {pred_probab}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0OCM8euheyjT"
      },
      "source": [
        "## Model Parameters\n",
        "================\n",
        "\n",
        "Many layers inside a neural network are *parameterized*, i.e. have\n",
        "associated weights and biases that are optimized during training.\n",
        "Subclassing `nn.Module` automatically tracks all fields defined inside\n",
        "your model object, and makes all parameters accessible using your\n",
        "model\\'s `parameters()` or `named_parameters()` methods.\n",
        "\n",
        "In this example, we iterate over each parameter, and print its size and\n",
        "a preview of its values.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchsummary import summary\n",
        "summary(model, input_size=(1, 28, 28))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dP1H1XVLofGp",
        "outputId": "cb306ee2-3822-40df-e65b-5f41c0f19077"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "           Flatten-1                  [-1, 784]               0\n",
            "            Linear-2                  [-1, 512]         401,920\n",
            "              ReLU-3                  [-1, 512]               0\n",
            "            Linear-4                  [-1, 512]         262,656\n",
            "              ReLU-5                  [-1, 512]               0\n",
            "            Linear-6                   [-1, 10]           5,130\n",
            "================================================================\n",
            "Total params: 669,706\n",
            "Trainable params: 669,706\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.02\n",
            "Params size (MB): 2.55\n",
            "Estimated Total Size (MB): 2.58\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mwzoI9nNeyjT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0967299-023f-48c6-d86b-725e1681287b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model structure: NeuralNetwork(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (fc1): Linear(in_features=784, out_features=512, bias=True)\n",
            "  (relu1): ReLU()\n",
            "  (fc2): Linear(in_features=512, out_features=512, bias=True)\n",
            "  (relu2): ReLU()\n",
            "  (fc3): Linear(in_features=512, out_features=10, bias=True)\n",
            ")\n",
            "\n",
            "\n",
            "Layer: fc1.weight | Size: torch.Size([512, 784]) | Values : tensor([[ 0.0139, -0.0218, -0.0087,  ..., -0.0075, -0.0204, -0.0097],\n",
            "        [-0.0317,  0.0154,  0.0182,  ...,  0.0105, -0.0199,  0.0330]],\n",
            "       device='cuda:0', grad_fn=<SliceBackward0>) \n",
            "\n",
            "Layer: fc1.bias | Size: torch.Size([512]) | Values : tensor([-0.0054, -0.0099], device='cuda:0', grad_fn=<SliceBackward0>) \n",
            "\n",
            "Layer: fc2.weight | Size: torch.Size([512, 512]) | Values : tensor([[-0.0117,  0.0358, -0.0192,  ...,  0.0155,  0.0077, -0.0388],\n",
            "        [-0.0037,  0.0182, -0.0024,  ..., -0.0126, -0.0123, -0.0243]],\n",
            "       device='cuda:0', grad_fn=<SliceBackward0>) \n",
            "\n",
            "Layer: fc2.bias | Size: torch.Size([512]) | Values : tensor([0.0043, 0.0134], device='cuda:0', grad_fn=<SliceBackward0>) \n",
            "\n",
            "Layer: fc3.weight | Size: torch.Size([10, 512]) | Values : tensor([[-0.0011, -0.0244,  0.0112,  ...,  0.0174, -0.0009,  0.0167],\n",
            "        [ 0.0231,  0.0393, -0.0120,  ..., -0.0042,  0.0233,  0.0144]],\n",
            "       device='cuda:0', grad_fn=<SliceBackward0>) \n",
            "\n",
            "Layer: fc3.bias | Size: torch.Size([10]) | Values : tensor([ 0.0246, -0.0203], device='cuda:0', grad_fn=<SliceBackward0>) \n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(f\"Model structure: {model}\\n\\n\")\n",
        "\n",
        "for name, param in model.named_parameters():\n",
        "    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wyGYYkXUo3Va"
      },
      "source": [
        "# Automatic Differentiation with `torch.autograd`\n",
        "\n",
        "When training neural networks, the most frequently used algorithm is\n",
        "**back propagation**. In this algorithm, parameters (model weights) are\n",
        "adjusted according to the **gradient** of the loss function with respect\n",
        "to the given parameter.\n",
        "\n",
        "To compute those gradients, PyTorch has a built-in differentiation\n",
        "engine called `torch.autograd`. It supports automatic computation of\n",
        "gradient for any computational graph.\n",
        "\n",
        "Consider the simplest one-layer neural network, with input `x`,\n",
        "parameters `w` and `b`, and some loss function. It can be defined in\n",
        "PyTorch in the following manner:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EkLI1lw_o3Vb"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "x = torch.ones(5)  # input tensor\n",
        "y = torch.zeros(3)  # expected output\n",
        "\n",
        "# requires_grad=True记录计算过程 反向计算梯度\n",
        "# 或者后期x.requires_grad_(True)\n",
        "w = torch.randn(5, 3, requires_grad=True)\n",
        "b = torch.randn(3, requires_grad=True)\n",
        "\n",
        "# 前向传播\n",
        "z = torch.matmul(x, w)+b # 矩阵乘法加偏置\n",
        "\n",
        "# 损失函数\n",
        "loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y) # 二元交叉熵"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xa55WRWmo3Vb"
      },
      "source": [
        "## Tensors, Functions and Computational graph\n",
        "\n",
        "This code defines the following **computational graph**:\n",
        "\n",
        "![](https://pytorch.org/tutorials/_static/img/basics/comp-graph.png)\n",
        "\n",
        "In this network, `w` and `b` are **parameters**, which we need to\n",
        "optimize. Thus, we need to be able to compute the gradients of loss\n",
        "function with respect to those variables. In order to do that, we set\n",
        "the `requires_grad` property of those tensors.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdp4rMZno3Vc"
      },
      "source": [
        "<div style=\"background-color: #54c7ec; color: #fff; font-weight: 700; padding-left: 10px; padding-top: 5px; padding-bottom: 5px\"><strong>NOTE:</strong></div>\n",
        "<div style=\"background-color: #f3f4f7; padding-left: 10px; padding-top: 10px; padding-bottom: 10px; padding-right: 10px\">\n",
        "<p>You can set the value of <code>requires_grad</code> when creating atensor, or later by using <code>x.requires_grad_(True)</code> method.</p>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MIwQ0O8o3Vc"
      },
      "source": [
        "A function that we apply to tensors to construct computational graph is\n",
        "in fact an object of class `Function`. This object knows how to compute\n",
        "the function in the *forward* direction, and also how to compute its\n",
        "derivative during the *backward propagation* step. A reference to the\n",
        "backward propagation function is stored in `grad_fn` property of a\n",
        "tensor. You can find more information of `Function` [in the\n",
        "documentation](https://pytorch.org/docs/stable/autograd.html#function).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k0yEfccyo3Vc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0b19ad7-2f92-4b73-e660-fa26a27f9f64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient function for z = <AddBackward0 object at 0x7f9a8219f310>\n",
            "Gradient function for loss = <BinaryCrossEntropyWithLogitsBackward0 object at 0x7f9a8219f9a0>\n"
          ]
        }
      ],
      "source": [
        "# 每个张量都通过grad_fn属性，记录了生成该张量的操作\n",
        "print(f\"Gradient function for z = {z.grad_fn}\")\n",
        "print(f\"Gradient function for loss = {loss.grad_fn}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 可视化grad_fn\n",
        "!pip install torchviz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2pnUf1AHzvam",
        "outputId": "34d429a6-9229-48d8-d05e-65348f333c19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchviz\n",
            "  Downloading torchviz-0.0.2.tar.gz (4.9 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from torchviz) (2.5.1+cu121)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from torchviz) (0.20.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->torchviz) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->torchviz) (3.0.2)\n",
            "Building wheels for collected packages: torchviz\n",
            "  Building wheel for torchviz (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torchviz: filename=torchviz-0.0.2-py3-none-any.whl size=4132 sha256=4541ce57d7d7d2e8efa2be065a766d9f1c305a419f7d25f784639929feb39741\n",
            "  Stored in directory: /root/.cache/pip/wheels/4c/97/88/a02973217949e0db0c9f4346d154085f4725f99c4f15a87094\n",
            "Successfully built torchviz\n",
            "Installing collected packages: torchviz\n",
            "Successfully installed torchviz-0.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchviz import make_dot\n",
        "\n",
        "x = torch.ones(5)\n",
        "y = torch.zeros(3)\n",
        "w = torch.randn(5, 3, requires_grad=True)\n",
        "b = torch.randn(3, requires_grad=True)\n",
        "\n",
        "z = torch.matmul(x, w) + b\n",
        "loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)\n",
        "\n",
        "# 使用 make_dot 可视化计算图\n",
        "dot = make_dot(z, params={'w': w, 'b': b})\n",
        "dot.render(\"computation_graph\", format=\"png\")  # 保存为 PNG 图片\n",
        "dot"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468
        },
        "id": "qKl4gCSIziRj",
        "outputId": "0e8647fb-3db6-47ba-a320-bcdfb2afccc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.43.0 (0)\n -->\n<!-- Title: %3 Pages: 1 -->\n<svg width=\"240pt\" height=\"336pt\"\n viewBox=\"0.00 0.00 240.00 336.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 332)\">\n<title>%3</title>\n<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-332 236,-332 236,4 -4,4\"/>\n<!-- 140301629159872 -->\n<g id=\"node1\" class=\"node\">\n<title>140301629159872</title>\n<polygon fill=\"#caff70\" stroke=\"black\" points=\"145.5,-31 91.5,-31 91.5,0 145.5,0 145.5,-31\"/>\n<text text-anchor=\"middle\" x=\"118.5\" y=\"-7\" font-family=\"monospace\" font-size=\"10.00\"> (3)</text>\n</g>\n<!-- 140301619655680 -->\n<g id=\"node2\" class=\"node\">\n<title>140301619655680</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"163,-86 74,-86 74,-67 163,-67 163,-86\"/>\n<text text-anchor=\"middle\" x=\"118.5\" y=\"-74\" font-family=\"monospace\" font-size=\"10.00\">AddBackward0</text>\n</g>\n<!-- 140301619655680&#45;&gt;140301629159872 -->\n<g id=\"edge7\" class=\"edge\">\n<title>140301619655680&#45;&gt;140301629159872</title>\n<path fill=\"none\" stroke=\"black\" d=\"M118.5,-66.79C118.5,-60.07 118.5,-50.4 118.5,-41.34\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"122,-41.19 118.5,-31.19 115,-41.19 122,-41.19\"/>\n</g>\n<!-- 140301619655392 -->\n<g id=\"node3\" class=\"node\">\n<title>140301619655392</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"113,-141 0,-141 0,-122 113,-122 113,-141\"/>\n<text text-anchor=\"middle\" x=\"56.5\" y=\"-129\" font-family=\"monospace\" font-size=\"10.00\">SqueezeBackward4</text>\n</g>\n<!-- 140301619655392&#45;&gt;140301619655680 -->\n<g id=\"edge1\" class=\"edge\">\n<title>140301619655392&#45;&gt;140301619655680</title>\n<path fill=\"none\" stroke=\"black\" d=\"M66.46,-121.98C75.63,-114.15 89.44,-102.34 100.53,-92.86\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"103.03,-95.33 108.36,-86.17 98.48,-90.01 103.03,-95.33\"/>\n</g>\n<!-- 140301619656496 -->\n<g id=\"node4\" class=\"node\">\n<title>140301619656496</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"98,-201.5 15,-201.5 15,-182.5 98,-182.5 98,-201.5\"/>\n<text text-anchor=\"middle\" x=\"56.5\" y=\"-189.5\" font-family=\"monospace\" font-size=\"10.00\">MmBackward0</text>\n</g>\n<!-- 140301619656496&#45;&gt;140301619655392 -->\n<g id=\"edge2\" class=\"edge\">\n<title>140301619656496&#45;&gt;140301619655392</title>\n<path fill=\"none\" stroke=\"black\" d=\"M56.5,-182.37C56.5,-174.25 56.5,-161.81 56.5,-151.39\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"60,-151.17 56.5,-141.17 53,-151.17 60,-151.17\"/>\n</g>\n<!-- 140301619643392 -->\n<g id=\"node5\" class=\"node\">\n<title>140301619643392</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"107,-262 6,-262 6,-243 107,-243 107,-262\"/>\n<text text-anchor=\"middle\" x=\"56.5\" y=\"-250\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n</g>\n<!-- 140301619643392&#45;&gt;140301619656496 -->\n<g id=\"edge3\" class=\"edge\">\n<title>140301619643392&#45;&gt;140301619656496</title>\n<path fill=\"none\" stroke=\"black\" d=\"M56.5,-242.87C56.5,-234.75 56.5,-222.31 56.5,-211.89\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"60,-211.67 56.5,-201.67 53,-211.67 60,-211.67\"/>\n</g>\n<!-- 140301715729968 -->\n<g id=\"node6\" class=\"node\">\n<title>140301715729968</title>\n<polygon fill=\"lightblue\" stroke=\"black\" points=\"86,-328 27,-328 27,-298 86,-298 86,-328\"/>\n<text text-anchor=\"middle\" x=\"56.5\" y=\"-316\" font-family=\"monospace\" font-size=\"10.00\">w</text>\n<text text-anchor=\"middle\" x=\"56.5\" y=\"-305\" font-family=\"monospace\" font-size=\"10.00\"> (5, 3)</text>\n</g>\n<!-- 140301715729968&#45;&gt;140301619643392 -->\n<g id=\"edge4\" class=\"edge\">\n<title>140301715729968&#45;&gt;140301619643392</title>\n<path fill=\"none\" stroke=\"black\" d=\"M56.5,-297.84C56.5,-290.21 56.5,-280.7 56.5,-272.45\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"60,-272.27 56.5,-262.27 53,-272.27 60,-272.27\"/>\n</g>\n<!-- 140301619655488 -->\n<g id=\"node7\" class=\"node\">\n<title>140301619655488</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"232,-141 131,-141 131,-122 232,-122 232,-141\"/>\n<text text-anchor=\"middle\" x=\"181.5\" y=\"-129\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n</g>\n<!-- 140301619655488&#45;&gt;140301619655680 -->\n<g id=\"edge5\" class=\"edge\">\n<title>140301619655488&#45;&gt;140301619655680</title>\n<path fill=\"none\" stroke=\"black\" d=\"M171.38,-121.98C162.06,-114.15 148.03,-102.34 136.76,-92.86\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"138.71,-89.93 128.81,-86.17 134.21,-95.29 138.71,-89.93\"/>\n</g>\n<!-- 140305290792768 -->\n<g id=\"node8\" class=\"node\">\n<title>140305290792768</title>\n<polygon fill=\"lightblue\" stroke=\"black\" points=\"208.5,-207 154.5,-207 154.5,-177 208.5,-177 208.5,-207\"/>\n<text text-anchor=\"middle\" x=\"181.5\" y=\"-195\" font-family=\"monospace\" font-size=\"10.00\">b</text>\n<text text-anchor=\"middle\" x=\"181.5\" y=\"-184\" font-family=\"monospace\" font-size=\"10.00\"> (3)</text>\n</g>\n<!-- 140305290792768&#45;&gt;140301619655488 -->\n<g id=\"edge6\" class=\"edge\">\n<title>140305290792768&#45;&gt;140301619655488</title>\n<path fill=\"none\" stroke=\"black\" d=\"M181.5,-176.84C181.5,-169.21 181.5,-159.7 181.5,-151.45\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"185,-151.27 181.5,-141.27 178,-151.27 185,-151.27\"/>\n</g>\n</g>\n</svg>\n",
            "text/plain": [
              "<graphviz.graphs.Digraph at 0x7f9a843373d0>"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. 蓝色节点（w 和 b）：\n",
        "模型的参数。\n",
        "w 和 b 分别是权重矩阵（w (5, 3)）和偏置向量（b (3)），它们都启用了梯度跟踪。\n",
        "梯度会通过反向传播累积到这些参数中。\n",
        "2. 灰色节点：\n",
        "每个灰色节点代表一个反向传播的操作（Backward 函数）。\n",
        "例如：\n",
        "* AccumulateGrad: 用于累积梯度到 w 和 b 的 .grad 属性中。\n",
        "* MmBackward0: 矩阵乘法的反向传播操作。\n",
        "* SqueezeBackward4: 与 z 的形状调整有关的操作。\n",
        "* AddBackward0: 对 z = matmul(x, w) + b 中的加法执行反向传播。\n",
        "3. 绿色节点（最终输出）：\n",
        "绿色节点表示计算图的最终输出张量，这里是损失值 loss。\n",
        "损失值通过反向传播开始，逐层向上计算梯度。"
      ],
      "metadata": {
        "id": "AkAXW1q_Cj31"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dot = make_dot(loss, params={'w': w, 'b': b})\n",
        "dot.render(\"computation_graph\", format=\"png\")  # 保存为 PNG 图片\n",
        "dot"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 541
        },
        "id": "7n85x4oNB1__",
        "outputId": "d9673e3d-acef-43f5-dd7e-d6a93a58fe48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.43.0 (0)\n -->\n<!-- Title: %3 Pages: 1 -->\n<svg width=\"247pt\" height=\"391pt\"\n viewBox=\"0.00 0.00 247.00 391.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 387)\">\n<title>%3</title>\n<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-387 243,-387 243,4 -4,4\"/>\n<!-- 140301584947840 -->\n<g id=\"node1\" class=\"node\">\n<title>140301584947840</title>\n<polygon fill=\"#caff70\" stroke=\"black\" points=\"146.5,-31 92.5,-31 92.5,0 146.5,0 146.5,-31\"/>\n<text text-anchor=\"middle\" x=\"119.5\" y=\"-7\" font-family=\"monospace\" font-size=\"10.00\"> ()</text>\n</g>\n<!-- 140301619654048 -->\n<g id=\"node2\" class=\"node\">\n<title>140301619654048</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"239,-86 0,-86 0,-67 239,-67 239,-86\"/>\n<text text-anchor=\"middle\" x=\"119.5\" y=\"-74\" font-family=\"monospace\" font-size=\"10.00\">BinaryCrossEntropyWithLogitsBackward0</text>\n</g>\n<!-- 140301619654048&#45;&gt;140301584947840 -->\n<g id=\"edge8\" class=\"edge\">\n<title>140301619654048&#45;&gt;140301584947840</title>\n<path fill=\"none\" stroke=\"black\" d=\"M119.5,-66.79C119.5,-60.07 119.5,-50.4 119.5,-41.34\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"123,-41.19 119.5,-31.19 116,-41.19 123,-41.19\"/>\n</g>\n<!-- 140301619653952 -->\n<g id=\"node3\" class=\"node\">\n<title>140301619653952</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"164,-141 75,-141 75,-122 164,-122 164,-141\"/>\n<text text-anchor=\"middle\" x=\"119.5\" y=\"-129\" font-family=\"monospace\" font-size=\"10.00\">AddBackward0</text>\n</g>\n<!-- 140301619653952&#45;&gt;140301619654048 -->\n<g id=\"edge1\" class=\"edge\">\n<title>140301619653952&#45;&gt;140301619654048</title>\n<path fill=\"none\" stroke=\"black\" d=\"M119.5,-121.75C119.5,-114.8 119.5,-104.85 119.5,-96.13\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"123,-96.09 119.5,-86.09 116,-96.09 123,-96.09\"/>\n</g>\n<!-- 140301619644304 -->\n<g id=\"node4\" class=\"node\">\n<title>140301619644304</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"114,-196 1,-196 1,-177 114,-177 114,-196\"/>\n<text text-anchor=\"middle\" x=\"57.5\" y=\"-184\" font-family=\"monospace\" font-size=\"10.00\">SqueezeBackward4</text>\n</g>\n<!-- 140301619644304&#45;&gt;140301619653952 -->\n<g id=\"edge2\" class=\"edge\">\n<title>140301619644304&#45;&gt;140301619653952</title>\n<path fill=\"none\" stroke=\"black\" d=\"M67.46,-176.98C76.63,-169.15 90.44,-157.34 101.53,-147.86\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"104.03,-150.33 109.36,-141.17 99.48,-145.01 104.03,-150.33\"/>\n</g>\n<!-- 140301619644256 -->\n<g id=\"node5\" class=\"node\">\n<title>140301619644256</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"99,-256.5 16,-256.5 16,-237.5 99,-237.5 99,-256.5\"/>\n<text text-anchor=\"middle\" x=\"57.5\" y=\"-244.5\" font-family=\"monospace\" font-size=\"10.00\">MmBackward0</text>\n</g>\n<!-- 140301619644256&#45;&gt;140301619644304 -->\n<g id=\"edge3\" class=\"edge\">\n<title>140301619644256&#45;&gt;140301619644304</title>\n<path fill=\"none\" stroke=\"black\" d=\"M57.5,-237.37C57.5,-229.25 57.5,-216.81 57.5,-206.39\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"61,-206.17 57.5,-196.17 54,-206.17 61,-206.17\"/>\n</g>\n<!-- 140301619644112 -->\n<g id=\"node6\" class=\"node\">\n<title>140301619644112</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"108,-317 7,-317 7,-298 108,-298 108,-317\"/>\n<text text-anchor=\"middle\" x=\"57.5\" y=\"-305\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n</g>\n<!-- 140301619644112&#45;&gt;140301619644256 -->\n<g id=\"edge4\" class=\"edge\">\n<title>140301619644112&#45;&gt;140301619644256</title>\n<path fill=\"none\" stroke=\"black\" d=\"M57.5,-297.87C57.5,-289.75 57.5,-277.31 57.5,-266.89\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"61,-266.67 57.5,-256.67 54,-266.67 61,-266.67\"/>\n</g>\n<!-- 140301715729968 -->\n<g id=\"node7\" class=\"node\">\n<title>140301715729968</title>\n<polygon fill=\"lightblue\" stroke=\"black\" points=\"87,-383 28,-383 28,-353 87,-353 87,-383\"/>\n<text text-anchor=\"middle\" x=\"57.5\" y=\"-371\" font-family=\"monospace\" font-size=\"10.00\">w</text>\n<text text-anchor=\"middle\" x=\"57.5\" y=\"-360\" font-family=\"monospace\" font-size=\"10.00\"> (5, 3)</text>\n</g>\n<!-- 140301715729968&#45;&gt;140301619644112 -->\n<g id=\"edge5\" class=\"edge\">\n<title>140301715729968&#45;&gt;140301619644112</title>\n<path fill=\"none\" stroke=\"black\" d=\"M57.5,-352.84C57.5,-345.21 57.5,-335.7 57.5,-327.45\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"61,-327.27 57.5,-317.27 54,-327.27 61,-327.27\"/>\n</g>\n<!-- 140301619644400 -->\n<g id=\"node8\" class=\"node\">\n<title>140301619644400</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"233,-196 132,-196 132,-177 233,-177 233,-196\"/>\n<text text-anchor=\"middle\" x=\"182.5\" y=\"-184\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n</g>\n<!-- 140301619644400&#45;&gt;140301619653952 -->\n<g id=\"edge6\" class=\"edge\">\n<title>140301619644400&#45;&gt;140301619653952</title>\n<path fill=\"none\" stroke=\"black\" d=\"M172.38,-176.98C163.06,-169.15 149.03,-157.34 137.76,-147.86\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"139.71,-144.93 129.81,-141.17 135.21,-150.29 139.71,-144.93\"/>\n</g>\n<!-- 140305290792768 -->\n<g id=\"node9\" class=\"node\">\n<title>140305290792768</title>\n<polygon fill=\"lightblue\" stroke=\"black\" points=\"209.5,-262 155.5,-262 155.5,-232 209.5,-232 209.5,-262\"/>\n<text text-anchor=\"middle\" x=\"182.5\" y=\"-250\" font-family=\"monospace\" font-size=\"10.00\">b</text>\n<text text-anchor=\"middle\" x=\"182.5\" y=\"-239\" font-family=\"monospace\" font-size=\"10.00\"> (3)</text>\n</g>\n<!-- 140305290792768&#45;&gt;140301619644400 -->\n<g id=\"edge7\" class=\"edge\">\n<title>140305290792768&#45;&gt;140301619644400</title>\n<path fill=\"none\" stroke=\"black\" d=\"M182.5,-231.84C182.5,-224.21 182.5,-214.7 182.5,-206.45\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"186,-206.27 182.5,-196.27 179,-206.27 186,-206.27\"/>\n</g>\n</g>\n</svg>\n",
            "text/plain": [
              "<graphviz.graphs.Digraph at 0x7f9a84336dd0>"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-TyTl7JRo3Vc"
      },
      "source": [
        "## Computing Gradients\n",
        "\n",
        "\n",
        "To optimize weights of parameters in the neural network, we need to\n",
        "compute the derivatives of our loss function with respect to parameters,\n",
        "namely, we need $\\frac{\\partial loss}{\\partial w}$ and\n",
        "$\\frac{\\partial loss}{\\partial b}$ under some fixed values of `x` and\n",
        "`y`. To compute those derivatives, we call `loss.backward()`, and then\n",
        "retrieve the values from `w.grad` and `b.grad`:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kWj_KcpGo3Vd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49f55f09-d8d5-4206-f124-9df107469324"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.2290, 0.2617, 0.2760],\n",
            "        [0.2290, 0.2617, 0.2760],\n",
            "        [0.2290, 0.2617, 0.2760],\n",
            "        [0.2290, 0.2617, 0.2760],\n",
            "        [0.2290, 0.2617, 0.2760]])\n",
            "tensor([0.2290, 0.2617, 0.2760])\n"
          ]
        }
      ],
      "source": [
        "# 梯度是通过调用loss.backward()自动计算的\n",
        "loss.backward()\n",
        "\n",
        "# 可以通过参数的.grad属性获取梯度值\n",
        "print(w.grad)\n",
        "print(b.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_W7-hgb6o3Vd"
      },
      "source": [
        "<div style=\"background-color: #54c7ec; color: #fff; font-weight: 700; padding-left: 10px; padding-top: 5px; padding-bottom: 5px\"><strong>NOTE:</strong></div>\n",
        "<div style=\"background-color: #f3f4f7; padding-left: 10px; padding-top: 10px; padding-bottom: 10px; padding-right: 10px\">\n",
        "<ul>\n",
        "<li>We can only obtain the <code>grad</code> properties for the leafnodes of the computational graph, which have <code>requires_grad</code> propertyset to <code>True</code>. For all other nodes in our graph, gradients will not beavailable.\n",
        "<li>leafnodes是指计算图中用户直接创建的张量。\n",
        "<li>We can only perform gradient calculations using<code>backward</code> once on a given graph, for performance reasons. If we needto do several <code>backward</code> calls on the same graph, we need to pass<code>retain_graph=True</code> to the <code>backward</code> call.</li>\n",
        "</ul>\n",
        "</div>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bC7lEHro3Vd"
      },
      "source": [
        "## Disabling Gradient Tracking\n",
        "\n",
        "\n",
        "By default, all tensors with `requires_grad=True` are tracking their\n",
        "computational history and support gradient computation. However, there\n",
        "are some cases when we do not need to do that, for example, when we have\n",
        "trained the model and just want to apply it to some input data, i.e. we\n",
        "only want to do *forward* computations through the network. We can stop\n",
        "tracking computations by surrounding our computation code with\n",
        "`torch.no_grad()` block:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "diZ1v9sqo3Vd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88e869b3-e301-4601-8c03-da76d7b59dde"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "False\n"
          ]
        }
      ],
      "source": [
        "# 方法1：torch.no_grad()是一个上下文管理器，用于临时禁用梯度计算。\n",
        "\n",
        "z = torch.matmul(x, w)+b\n",
        "print(z.requires_grad)\n",
        "\n",
        "with torch.no_grad():\n",
        "    z = torch.matmul(x, w)+b\n",
        "print(z.requires_grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6duxK8Gho3Vd"
      },
      "source": [
        "Another way to achieve the same result is to use the `detach()` method\n",
        "on the tensor:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qIQyY2ujo3Ve",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4563d307-5373-4752-ea3e-3a7b12f53f8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n"
          ]
        }
      ],
      "source": [
        "# 方法2：使用detach()方法\n",
        "z = torch.matmul(x, w)+b\n",
        "# detach()方法会返回一个新的张量，与原始张量共享数据内存，但不会记录计算历史。\n",
        "z_det = z.detach()\n",
        "print(z_det.requires_grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cw9KgbBgo3Ve"
      },
      "source": [
        "There are reasons you might want to disable gradient tracking:\n",
        "\n",
        "1. To mark some parameters in your neural network as **frozen parameters**.\n",
        "2. To **speed up computations** when you are only doing forward pass, because computations on tensors that do not track gradients would be more efficient.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qlkeHWjvo3Ve"
      },
      "source": [
        "## More on Computational Graphs\n",
        "\n",
        "Conceptually, autograd keeps a record of data (tensors) and all executed\n",
        "operations (along with the resulting new tensors) in a directed acyclic\n",
        "graph (DAG 有向无环图) consisting of\n",
        "[Function](https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
        "objects. In this DAG, leaves are the input tensors, roots are the output\n",
        "tensors. By tracing this graph from roots to leaves, you can\n",
        "automatically compute the gradients using the chain rule(链式法则).\n",
        "\n",
        "In a forward pass, autograd does two things simultaneously:\n",
        "\n",
        "-   执行：run the requested operation to compute a resulting tensor\n",
        "-   记录：maintain the operation's *gradient function* in the DAG.\n",
        "\n",
        "The backward pass kicks off when `.backward()` is called on the DAG\n",
        "root. `autograd` then:\n",
        "\n",
        "-   计算：computes the gradients from each `.grad_fn`,\n",
        "-   累计梯度：accumulates them in the respective tensor's `.grad` attribute\n",
        "-   链式：using the chain rule, propagates all the way to the leaf tensors.\n",
        "\n",
        "<div style=\"background-color: #54c7ec; color: #fff; font-weight: 700; padding-left: 10px; padding-top: 5px; padding-bottom: 5px\"><strong>NOTE:</strong></div>\n",
        "<div style=\"background-color: #f3f4f7; padding-left: 10px; padding-top: 10px; padding-bottom: 10px; padding-right: 10px\">\n",
        "<p>An important thing to note is that the graph is recreated from scratch; after each<code>.backward()</code> call, autograd starts populating a new graph. This isexactly what allows you to use control flow statements in your model;you can change the shape, size and operations at every iteration ifneeded.</p>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Otv8iZvo3Ve"
      },
      "source": [
        "## Optional Reading: Tensor Gradients and Jacobian Products\n",
        "\n",
        "In many cases, we have a scalar loss function, and we need to compute\n",
        "the gradient with respect to some parameters. However, there are cases\n",
        "when **the output function is an arbitrary tensor**. In this case, PyTorch\n",
        "allows you to compute so-called **Jacobian product**, and not the actual\n",
        "gradient.\n",
        "\n",
        "For a vector function $\\vec{y}=f(\\vec{x})$, where\n",
        "$\\vec{x}=\\langle x_1,\\dots,x_n\\rangle$ and\n",
        "$\\vec{y}=\\langle y_1,\\dots,y_m\\rangle$, a gradient of $\\vec{y}$ with\n",
        "respect to $\\vec{x}$ is given by **Jacobian matrix**:\n",
        "\n",
        "$$\\begin{aligned}\n",
        "J=\\left(\\begin{array}{ccc}\n",
        "   \\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{1}}{\\partial x_{n}}\\\\\n",
        "   \\vdots & \\ddots & \\vdots\\\\\n",
        "   \\frac{\\partial y_{m}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}}\n",
        "   \\end{array}\\right)\n",
        "\\end{aligned}$$\n",
        "\n",
        "Instead of computing the Jacobian matrix itself, PyTorch allows you to\n",
        "compute **Jacobian Product** $v^T\\cdot J$ for a given input vector\n",
        "$v=(v_1 \\dots v_m)$.(通过这种方式，可以避免构造完整的雅可比矩阵，节省内存。) This is achieved by calling `backward` with $v$ as\n",
        "an argument. The size of $v$ should be the same as the size of the\n",
        "original tensor, with respect to which we want to compute the product:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6a7nAQ12o3Ve",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "823b6734-b8bd-47c7-f810-124537b8aa44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First call\n",
            "tensor([[4., 2., 2., 2., 2.],\n",
            "        [2., 4., 2., 2., 2.],\n",
            "        [2., 2., 4., 2., 2.],\n",
            "        [2., 2., 2., 4., 2.]])\n",
            "\n",
            "Second call\n",
            "tensor([[8., 4., 4., 4., 4.],\n",
            "        [4., 8., 4., 4., 4.],\n",
            "        [4., 4., 8., 4., 4.],\n",
            "        [4., 4., 4., 8., 4.]])\n",
            "\n",
            "Call after zeroing gradients\n",
            "tensor([[4., 2., 2., 2., 2.],\n",
            "        [2., 4., 2., 2., 2.],\n",
            "        [2., 2., 4., 2., 2.],\n",
            "        [2., 2., 2., 4., 2.]])\n"
          ]
        }
      ],
      "source": [
        "inp = torch.eye(4, 5, requires_grad=True) # 创建4x5单位矩阵，启用梯度\n",
        "out = (inp+1).pow(2).t() # 每个元素加1，平方，最后转置\n",
        "\n",
        "out.backward(torch.ones_like(out), retain_graph=True)\n",
        "print(f\"First call\\n{inp.grad}\")\n",
        "\n",
        "# 注意：第二次调用backward()时，梯度值会累积。\n",
        "# PyTorch默认会将新计算的梯度加到.grad属性中，而不是覆盖。\n",
        "# 梯度累计机制可以支持小批量梯度下降，多任务学习或多来源梯度优化，提供对梯度的后处理灵活性。\n",
        "out.backward(torch.ones_like(out), retain_graph=True)\n",
        "print(f\"\\nSecond call\\n{inp.grad}\")\n",
        "\n",
        "inp.grad.zero_() # 清零\n",
        "out.backward(torch.ones_like(out), retain_graph=True)\n",
        "print(f\"\\nCall after zeroing gradients\\n{inp.grad}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3EtYSjWo3Ve"
      },
      "source": [
        "Notice that when we call `backward` for the second time with the same\n",
        "argument, the value of the gradient is different. This happens because\n",
        "when doing `backward` propagation, PyTorch **accumulates the\n",
        "gradients**, i.e. the value of computed gradients is added to the `grad`\n",
        "property of all leaf nodes of computational graph. If you want to\n",
        "compute the proper gradients, you need to zero out the `grad` property\n",
        "before. In real-life training an *optimizer* helps us to do this.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6r9qBCho3Ve"
      },
      "source": [
        "<div style=\"background-color: #54c7ec; color: #fff; font-weight: 700; padding-left: 10px; padding-top: 5px; padding-bottom: 5px\"><strong>NOTE:</strong></div>\n",
        "<div style=\"background-color: #f3f4f7; padding-left: 10px; padding-top: 10px; padding-bottom: 10px; padding-right: 10px\">\n",
        "<p>Previously we were calling <code>backward()</code> function withoutparameters. This is essentially equivalent to calling<code>backward(torch.tensor(1.0))</code>, which is a useful way to compute thegradients in case of a scalar-valued function, such as loss duringneural network training.</p>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZLbWU9Wuo3Ve"
      },
      "source": [
        "------------------------------------------------------------------------\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-n5bd5uo3Ve"
      },
      "source": [
        "## Further Reading\n",
        "\n",
        "-   [Autograd\n",
        "    Mechanics](https://pytorch.org/docs/stable/notes/autograd.html)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9i6BGyE4lFJ"
      },
      "source": [
        "# Optimizing Model Parameters\n",
        "\n",
        "\n",
        "Now that we have a model and data it\\'s time to train, validate and test\n",
        "our model by optimizing its parameters on our data. Training a model is\n",
        "an iterative process; in each iteration the model makes a guess about\n",
        "the output, calculates the error in its guess (*loss*), collects the\n",
        "derivatives of the error with respect to its parameters (as we saw in\n",
        "the [previous section](autograd_tutorial.html)), and **optimizes** these\n",
        "parameters using gradient descent. For a more detailed walkthrough of\n",
        "this process, check out this video on [backpropagation from\n",
        "3Blue1Brown](https://www.youtube.com/watch?v=tIeHLnjs5U8).\n",
        "\n",
        "Prerequisite Code\n",
        "-----------------\n",
        "\n",
        "We load the code from the previous sections on [Datasets &\n",
        "DataLoaders](data_tutorial.html) and [Build\n",
        "Model](buildmodel_tutorial.html).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imQtjCxL4lFL"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "training_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "# Minibatch\n",
        "train_dataloader = DataLoader(training_data, batch_size=64)\n",
        "test_dataloader = DataLoader(test_data, batch_size=64)\n",
        "# train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True, num_workers=4)\n",
        "\n",
        "# Define\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(28*28, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 10),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "model = NeuralNetwork()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMkhVB1I4lFL"
      },
      "source": [
        "## Hyperparameters\n",
        "\n",
        "Hyperparameters are adjustable parameters that let you control the model\n",
        "optimization process. Different hyperparameter values can impact model\n",
        "training and convergence rates ([read\n",
        "more](https://pytorch.org/tutorials/beginner/hyperparameter_tuning_tutorial.html)\n",
        "about hyperparameter tuning)\n",
        "\n",
        "We define the following hyperparameters for training:\n",
        "\n",
        "**Number of Epochs** - the number times to iterate over the dataset\n",
        "\n",
        "**Batch Size** - the number of data samples propagated through the network before the parameters are updated\n",
        "\n",
        "**Learning Rate** - how much to update models parameters at each\n",
        "        batch/epoch. Smaller values yield slow learning speed, while\n",
        "        large values may result in unpredictable behavior during\n",
        "        training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RBeDzALj4lFL"
      },
      "outputs": [],
      "source": [
        "learning_rate = 1e-3\n",
        "batch_size = 64\n",
        "epochs = 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eX0G4q8H4lFM"
      },
      "source": [
        "## Optimization Loop\n",
        "\n",
        "Once we set our hyperparameters, we can then train and optimize our\n",
        "model with an optimization loop. Each iteration of the optimization loop\n",
        "is called an **epoch**.\n",
        "\n",
        "Each epoch consists of two main parts:\n",
        "\n",
        "**The Train Loop** - iterate over the training dataset and try\n",
        "        to converge to optimal parameters.\n",
        "\n",
        "**The Validation/Test Loop** - iterate over the test dataset to\n",
        "        check if model performance is improving.\n",
        "\n",
        "Let\\'s briefly familiarize ourselves with some of the concepts used in\n",
        "the training loop. Jump ahead to see the\n",
        "`full-impl-label`{.interpreted-text role=\"ref\"} of the optimization\n",
        "loop.\n",
        "\n",
        "Loss Function\n",
        "-------------\n",
        "\n",
        "When presented with some training data, our untrained network is likely\n",
        "not to give the correct answer. **Loss function** measures the degree of\n",
        "dissimilarity of obtained result to the target value, and it is the loss\n",
        "function that we want to minimize during training. To calculate the loss\n",
        "we make a prediction using the inputs of our given data sample and\n",
        "compare it against the true data label value.\n",
        "\n",
        "Common loss functions include\n",
        "[nn.MSELoss](https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html#torch.nn.MSELoss)\n",
        "(Mean Square Error) for regression tasks, and\n",
        "[nn.NLLLoss](https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html#torch.nn.NLLLoss)\n",
        "(Negative Log Likelihood) for classification.\n",
        "[nn.CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss)\n",
        "combines `nn.LogSoftmax` and `nn.NLLLoss`.\n",
        "\n",
        "We pass our model\\'s output logits to `nn.CrossEntropyLoss`, which will\n",
        "normalize the logits and compute the prediction error.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eFgItSWs4lFM"
      },
      "outputs": [],
      "source": [
        "# Initialize the loss function\n",
        "loss_fn = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOumhZcS4lFM"
      },
      "source": [
        "## Optimizer\n",
        "\n",
        "Optimization is the process of adjusting model parameters to reduce\n",
        "model error in each training step. **Optimization algorithms** define\n",
        "how this process is performed (in this example we use Stochastic\n",
        "Gradient Descent). All optimization logic is encapsulated in the\n",
        "`optimizer` object. Here, we use the SGD optimizer; additionally, there\n",
        "are many [different\n",
        "optimizers](https://pytorch.org/docs/stable/optim.html) available in\n",
        "PyTorch such as ADAM and RMSProp, that work better for different kinds\n",
        "of models and data.\n",
        "\n",
        "We initialize the optimizer by registering the model\\'s parameters that\n",
        "need to be trained, and passing in the learning rate hyperparameter.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d9s_-6xP4lFM"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "# Adam,RMSprop,Adagrad"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "如何选择优化器\n",
        "1. 基于数据特性:\n",
        "\n",
        "稀疏数据:使用 Adagrad 或 Adam。\n",
        "\n",
        "非平稳目标函数:使用 RMSprop 或 Adam。\n",
        "\n",
        "高维连续数据:使用 SGD（配合动量）。\n",
        "\n",
        "2. 基于任务:\n",
        "\n",
        "卷积神经网络（CNN）:SGD + 动量（通常表现较好）。Adam 也是常见选择。\n",
        "\n",
        "循环神经网络（RNN/LSTM）:RMSprop 或 Adam。\n",
        "\n",
        "微调预训练模型:AdamW，因其对权重衰减的处理更稳定。"
      ],
      "metadata": {
        "id": "kU03HBF-Ou6T"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N43BWBZi4lFN"
      },
      "source": [
        "Inside the training loop, optimization happens in three steps:\n",
        "\n",
        "* Call `optimizer.zero_grad()` to reset the gradients of model parameters. Gradients by default add up; to prevent double-counting, we explicitly zero them at each iteration.\n",
        "\n",
        "* Backpropagate the prediction loss with a call to `loss.backward()`. PyTorch deposits the gradients of the loss w.r.t. each parameter.\n",
        "\n",
        "* Once we have our gradients, we call `optimizer.step()` to adjust the parameters by the gradients collected in the backward pass.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "klprGTUD4lFN"
      },
      "source": [
        "## Full Implementation\n",
        "\n",
        "We define `train_loop` that loops over our optimization code, and\n",
        "`test_loop` that evaluates the model\\'s performance against our test\n",
        "data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HCqMUuqt4lFN"
      },
      "outputs": [],
      "source": [
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    # Set the model to training mode - important for batch normalization and dropout layers\n",
        "    # Unnecessary in this situation but added for best practices\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        # Compute prediction and loss\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward() # 反向传播，计算梯度\n",
        "        optimizer.step() # 使用优化器更新模型参数\n",
        "        optimizer.zero_grad() # 清零梯度，避免梯度累积\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), batch * batch_size + len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "\n",
        "def test_loop(dataloader, model, loss_fn):\n",
        "    # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
        "    # Unnecessary in this situation but added for best practices\n",
        "    model.eval()\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    test_loss, correct = 0, 0\n",
        "\n",
        "    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
        "    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
        "    with torch.no_grad(): # 禁用梯度计算（节省内存并加速计算）\n",
        "        for X, y in dataloader:\n",
        "            pred = model(X) # 前向传播，计算预测值\n",
        "            test_loss += loss_fn(pred, y).item() # 累加损失值\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item() # 计算正确预测的样本数\n",
        "\n",
        "    # 计算平均损失和准确率\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2Q0o3Us4lFN"
      },
      "source": [
        "We initialize the loss function and optimizer, and pass it to\n",
        "`train_loop` and `test_loop`. Feel free to increase the number of epochs\n",
        "to track the model\\'s improving performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-mP87gI84lFN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5168da9-6e0b-44f7-d21d-82a2e0682fef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.307666  [   64/60000]\n",
            "loss: 2.294008  [ 6464/60000]\n",
            "loss: 2.276087  [12864/60000]\n",
            "loss: 2.257209  [19264/60000]\n",
            "loss: 2.248857  [25664/60000]\n",
            "loss: 2.207702  [32064/60000]\n",
            "loss: 2.213538  [38464/60000]\n",
            "loss: 2.179087  [44864/60000]\n",
            "loss: 2.173460  [51264/60000]\n",
            "loss: 2.132231  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 45.9%, Avg loss: 2.136124 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 2.153716  [   64/60000]\n",
            "loss: 2.143166  [ 6464/60000]\n",
            "loss: 2.081985  [12864/60000]\n",
            "loss: 2.093915  [19264/60000]\n",
            "loss: 2.056637  [25664/60000]\n",
            "loss: 1.980167  [32064/60000]\n",
            "loss: 2.008082  [38464/60000]\n",
            "loss: 1.929737  [44864/60000]\n",
            "loss: 1.935623  [51264/60000]\n",
            "loss: 1.857150  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 58.9%, Avg loss: 1.860050 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 1.900856  [   64/60000]\n",
            "loss: 1.870243  [ 6464/60000]\n",
            "loss: 1.747531  [12864/60000]\n",
            "loss: 1.787465  [19264/60000]\n",
            "loss: 1.684533  [25664/60000]\n",
            "loss: 1.630483  [32064/60000]\n",
            "loss: 1.650357  [38464/60000]\n",
            "loss: 1.552562  [44864/60000]\n",
            "loss: 1.583833  [51264/60000]\n",
            "loss: 1.468122  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 60.9%, Avg loss: 1.487851 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 1.563072  [   64/60000]\n",
            "loss: 1.525135  [ 6464/60000]\n",
            "loss: 1.372068  [12864/60000]\n",
            "loss: 1.445415  [19264/60000]\n",
            "loss: 1.320598  [25664/60000]\n",
            "loss: 1.322423  [32064/60000]\n",
            "loss: 1.335831  [38464/60000]\n",
            "loss: 1.259887  [44864/60000]\n",
            "loss: 1.304667  [51264/60000]\n",
            "loss: 1.196179  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 63.4%, Avg loss: 1.222634 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 1.305949  [   64/60000]\n",
            "loss: 1.283283  [ 6464/60000]\n",
            "loss: 1.117875  [12864/60000]\n",
            "loss: 1.226184  [19264/60000]\n",
            "loss: 1.092929  [25664/60000]\n",
            "loss: 1.127630  [32064/60000]\n",
            "loss: 1.146817  [38464/60000]\n",
            "loss: 1.083367  [44864/60000]\n",
            "loss: 1.133500  [51264/60000]\n",
            "loss: 1.040758  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 64.8%, Avg loss: 1.062455 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 1.137547  [   64/60000]\n",
            "loss: 1.135723  [ 6464/60000]\n",
            "loss: 0.955650  [12864/60000]\n",
            "loss: 1.091947  [19264/60000]\n",
            "loss: 0.957845  [25664/60000]\n",
            "loss: 0.999788  [32064/60000]\n",
            "loss: 1.031654  [38464/60000]\n",
            "loss: 0.974694  [44864/60000]\n",
            "loss: 1.025151  [51264/60000]\n",
            "loss: 0.944998  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 66.1%, Avg loss: 0.961341 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 1.022672  [   64/60000]\n",
            "loss: 1.042054  [ 6464/60000]\n",
            "loss: 0.847016  [12864/60000]\n",
            "loss: 1.003762  [19264/60000]\n",
            "loss: 0.873958  [25664/60000]\n",
            "loss: 0.910792  [32064/60000]\n",
            "loss: 0.956394  [38464/60000]\n",
            "loss: 0.905066  [44864/60000]\n",
            "loss: 0.951526  [51264/60000]\n",
            "loss: 0.881319  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 67.3%, Avg loss: 0.893031 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 0.939025  [   64/60000]\n",
            "loss: 0.977431  [ 6464/60000]\n",
            "loss: 0.769837  [12864/60000]\n",
            "loss: 0.942062  [19264/60000]\n",
            "loss: 0.818209  [25664/60000]\n",
            "loss: 0.846084  [32064/60000]\n",
            "loss: 0.903145  [38464/60000]\n",
            "loss: 0.858373  [44864/60000]\n",
            "loss: 0.898548  [51264/60000]\n",
            "loss: 0.835654  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 68.6%, Avg loss: 0.843877 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 0.874875  [   64/60000]\n",
            "loss: 0.929013  [ 6464/60000]\n",
            "loss: 0.711967  [12864/60000]\n",
            "loss: 0.896618  [19264/60000]\n",
            "loss: 0.778224  [25664/60000]\n",
            "loss: 0.797452  [32064/60000]\n",
            "loss: 0.862566  [38464/60000]\n",
            "loss: 0.825258  [44864/60000]\n",
            "loss: 0.858461  [51264/60000]\n",
            "loss: 0.800759  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 70.0%, Avg loss: 0.806454 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 0.823315  [   64/60000]\n",
            "loss: 0.889996  [ 6464/60000]\n",
            "loss: 0.666435  [12864/60000]\n",
            "loss: 0.861712  [19264/60000]\n",
            "loss: 0.747750  [25664/60000]\n",
            "loss: 0.760007  [32064/60000]\n",
            "loss: 0.829821  [38464/60000]\n",
            "loss: 0.800302  [44864/60000]\n",
            "loss: 0.826781  [51264/60000]\n",
            "loss: 0.772939  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 71.3%, Avg loss: 0.776498 \n",
            "\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "epochs = 10\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
        "    test_loop(test_dataloader, model, loss_fn)\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HdBS2dtJ4lFN"
      },
      "source": [
        "## Further Reading\n",
        "\n",
        "-   [Loss\n",
        "    Functions](https://pytorch.org/docs/stable/nn.html#loss-functions)\n",
        "-   [torch.optim](https://pytorch.org/docs/stable/optim.html)\n",
        "-   [Warmstart Training a\n",
        "    Model](https://pytorch.org/tutorials/recipes/recipes/warmstarting_model_using_parameters_from_a_different_model.html)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANRnUxUMtAWq"
      },
      "source": [
        "# Save & Load Model\n",
        "\n",
        "Save and Load the Model\n",
        "=======================\n",
        "\n",
        "In this section we will look at how to persist model state with saving,\n",
        "loading and running model predictions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xfb8nvIvtAWq"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision.models as models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nC-w2A0jtAWr"
      },
      "source": [
        "## Saving and Loading Model Weights\n",
        "\n",
        "PyTorch models store the learned parameters in an internal state\n",
        "dictionary, called `state_dict`. These can be persisted via the\n",
        "`torch.save` method:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_lmOVeSwtAWr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7c69554-7cd6-4f48-98f3-14630280b828"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n",
            "100%|██████████| 528M/528M [00:08<00:00, 67.6MB/s]\n"
          ]
        }
      ],
      "source": [
        "model = models.vgg16(weights='IMAGENET1K_V1') # 加载预训练 VGG16 模型\n",
        "# model.state_dict()只保存权重\n",
        "torch.save(model.state_dict(), 'model_weights.pth') # 保存模型权重到文件"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IvXBMotytAWr"
      },
      "source": [
        "To load model weights, you need to create an instance of the same model\n",
        "first, and then load the parameters using `load_state_dict()` method.\n",
        "\n",
        "In the code below, we set `weights_only=True` to limit the functions\n",
        "executed during unpickling to only those necessary for loading weights.\n",
        "Using `weights_only=True` is considered a best practice when loading\n",
        "weights.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_FdpEu6atAWr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f371d5c-05bc-4a2d-8bb3-95562e50019b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VGG(\n",
              "  (features): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (3): ReLU(inplace=True)\n",
              "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (6): ReLU(inplace=True)\n",
              "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (8): ReLU(inplace=True)\n",
              "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (11): ReLU(inplace=True)\n",
              "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (13): ReLU(inplace=True)\n",
              "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (15): ReLU(inplace=True)\n",
              "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (18): ReLU(inplace=True)\n",
              "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (20): ReLU(inplace=True)\n",
              "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (22): ReLU(inplace=True)\n",
              "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (25): ReLU(inplace=True)\n",
              "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (27): ReLU(inplace=True)\n",
              "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (29): ReLU(inplace=True)\n",
              "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
              "  (classifier): Sequential(\n",
              "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Dropout(p=0.5, inplace=False)\n",
              "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "    (4): ReLU(inplace=True)\n",
              "    (5): Dropout(p=0.5, inplace=False)\n",
              "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "model = models.vgg16() # 创建一个未训练的 VGG16 模型\n",
        "\n",
        "# model.load_state_dict()：将加载的权重填充到模型中。\n",
        "# weights_only=True 限制解压缩操作，仅加载权重，避免额外执行不必要的代码\n",
        "model.load_state_dict(torch.load('model_weights.pth', weights_only=True)) # 加载保存的权重\n",
        "\n",
        "model.eval() # 将模型设置为评估模式\n",
        "# Dropout层会被禁用。\n",
        "# BatchNorm会使用全局统计量。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmZ5wRCbtAWr"
      },
      "source": [
        "<div style=\"background-color: #54c7ec; color: #fff; font-weight: 700; padding-left: 10px; padding-top: 5px; padding-bottom: 5px\"><strong>NOTE:</strong></div>\n",
        "\n",
        "<div style=\"background-color: #f3f4f7; padding-left: 10px; padding-top: 10px; padding-bottom: 10px; padding-right: 10px\">\n",
        "\n",
        "<p>be sure to call <code>model.eval()</code> method before inferencing to set the dropout and batch normalization layers to evaluation mode. Failing to do this will yield inconsistent inference results.</p>\n",
        "\n",
        "</div>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DO_ZWPxjtAWr"
      },
      "source": [
        "## Saving and Loading Models with Shapes\n",
        "\n",
        "When loading model weights, we needed to instantiate the model class\n",
        "first, because the class defines the structure of a network. We might\n",
        "want to save the structure of this class together with the model, in\n",
        "which case we can pass `model` (and not `model.state_dict()`) to the\n",
        "saving function:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6C4SDYuHtAWs"
      },
      "outputs": [],
      "source": [
        "# 保存整个模型\n",
        "torch.save(model, 'model.pth')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hvS9jCTtAWs"
      },
      "source": [
        "We can then load the model as demonstrated below.\n",
        "\n",
        "As described in [Saving and loading\n",
        "torch.nn.Modules](https://pytorch.org/docs/main/notes/serialization.html#saving-and-loading-torch-nn-modules),\n",
        "saving `state_dict` is considered the best practice. However, below we\n",
        "use `weights_only=False` because this involves loading the model, which\n",
        "is a legacy use case for `torch.save`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Syc4TvU6tAWs"
      },
      "outputs": [],
      "source": [
        "# 不只保存权重\n",
        "model = torch.load('model.pth', weights_only=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNHfY66WtAWs"
      },
      "source": [
        "<div style=\"background-color: #54c7ec; color: #fff; font-weight: 700; padding-left: 10px; padding-top: 5px; padding-bottom: 5px\"><strong>NOTE:</strong></div>\n",
        "\n",
        "<div style=\"background-color: #f3f4f7; padding-left: 10px; padding-top: 10px; padding-bottom: 10px; padding-right: 10px\">\n",
        "\n",
        "<p>This approach uses Python <a href=\"https://docs.python.org/3/library/pickle.html\">pickle</a> module when serializing the model, thus it relies on the actual class definition to be available when loading the model.</p>\n",
        "\n",
        "</div>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2FoMY6-vtAWs"
      },
      "source": [
        "## Related Tutorials\n",
        "\n",
        "-   [Saving and Loading a General Checkpoint in\n",
        "    PyTorch](https://pytorch.org/tutorials/recipes/recipes/saving_and_loading_a_general_checkpoint.html)\n",
        "-   [Tips for loading an nn.Module from a\n",
        "    checkpoint](https://pytorch.org/tutorials/recipes/recipes/module_load_state_dict_tips.html?highlight=loading%20nn%20module%20from%20checkpoint)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AiKymOjreyjT"
      },
      "source": [
        "------------------------------------------------------------------------\n"
      ]
    }
  ]
}