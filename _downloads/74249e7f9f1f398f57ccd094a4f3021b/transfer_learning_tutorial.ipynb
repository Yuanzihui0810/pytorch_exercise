{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1zGc92Aw85PY"
      },
      "outputs": [],
      "source": [
        "# For tips on running notebooks in Google Colab, see\n",
        "# https://pytorch.org/tutorials/beginner/colab\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vU2_HA_g85PY"
      },
      "source": [
        "Transfer Learning for Computer Vision Tutorial\n",
        "==============================================\n",
        "\n",
        "**Author**: [Sasank Chilamkurthy](https://chsasank.github.io)\n",
        "\n",
        "In this tutorial, you will learn how to train a convolutional neural\n",
        "network for image classification using transfer learning. You can read\n",
        "more about the transfer learning at [cs231n\n",
        "notes](https://cs231n.github.io/transfer-learning/)\n",
        "\n",
        "Quoting these notes,\n",
        "\n",
        "> In practice, very few people train an entire Convolutional Network\n",
        "> from scratch (with random initialization), because it is relatively\n",
        "> rare to have a dataset of sufficient size. Instead, it is common to\n",
        "> pretrain a ConvNet on a very large dataset (e.g. ImageNet, which\n",
        "> contains 1.2 million images with 1000 categories), and then use the\n",
        "> ConvNet either as an initialization or a fixed feature extractor for\n",
        "> the task of interest.\n",
        "\n",
        "These two major transfer learning scenarios look as follows:\n",
        "\n",
        "-   **Finetuning the ConvNet**: Instead of random initialization, we\n",
        "    initialize the network with a pretrained network, like the one that\n",
        "    is trained on imagenet 1000 dataset. Rest of the training looks as\n",
        "    usual.\n",
        "-   **ConvNet as fixed feature extractor**: Here, we will freeze the\n",
        "    weights for all of the network except that of the final fully\n",
        "    connected layer. This last fully connected layer is replaced with a\n",
        "    new one with random weights and only this layer is trained.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "dH5LkN0Z85Pa",
        "outputId": "3a52f74f-1614-4fa9-83b8-7c8d25317e08",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<contextlib.ExitStack at 0x7bd927411930>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# License: BSD\n",
        "# Author: Sasank Chilamkurthy\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim #提供优化器模块（如 SGD, Adam），用于更新模型参数\n",
        "from torch.optim import lr_scheduler #动态调整优化器的学习率\n",
        "import torch.backends.cudnn as cudnn #cudnn: NVIDIA 的GPU加速库，用于加速深度学习中的卷积计算\n",
        "import numpy as np\n",
        "import torchvision\n",
        "# datasets: 提供常见数据集加载工具（如 CIFAR, ImageNet）\n",
        "# models：包含预训练模型（如 ResNet, VGG）\n",
        "# transforms：用于数据增强和预处理\n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "from PIL import Image #Python 图像处理库，用于加载和操作图像数据\n",
        "from tempfile import TemporaryDirectory #临时文件夹模块，适合在测试阶段存储临时文件或模型\n",
        "\n",
        "cudnn.benchmark = True #通过启用 cuDNN 的自动调优功能，加速卷积运算\n",
        "plt.ion()   # interactive mode\n",
        "# 使Matplotlib能够实时更新绘图窗口。适用于在训练过程中实时显示数据（例如损失、准确率）"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08U5Igka85Pa"
      },
      "source": [
        "Load Data\n",
        "=========\n",
        "\n",
        "We will use torchvision and torch.utils.data packages for loading the\n",
        "data.\n",
        "\n",
        "The problem we\\'re going to solve today is to train a model to classify\n",
        "**ants** and **bees**. We have about 120 training images each for ants\n",
        "and bees. There are 75 validation images for each class. Usually, this\n",
        "is a very small dataset to generalize upon, if trained from scratch.\n",
        "Since we are using transfer learning, we should be able to generalize\n",
        "reasonably well.\n",
        "\n",
        "This dataset is a very small subset of imagenet.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "m1Iu_TKk85Pa",
        "outputId": "bb00e14b-8977-456b-b12f-e6822e6355c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'data/hymenoptera_data/train'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-e997305388f6>\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mdata_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'data/hymenoptera_data'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n\u001b[0m\u001b[1;32m     20\u001b[0m                                           data_transforms[x])\n\u001b[1;32m     21\u001b[0m                   for x in ['train', 'val']}\n",
            "\u001b[0;32m<ipython-input-3-e997305388f6>\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mdata_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'data/hymenoptera_data'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n\u001b[0m\u001b[1;32m     20\u001b[0m                                           data_transforms[x])\n\u001b[1;32m     21\u001b[0m                   for x in ['train', 'val']}\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file, allow_empty)\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[0mallow_empty\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m     ):\n\u001b[0;32m--> 328\u001b[0;31m         super().__init__(\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file, allow_empty)\u001b[0m\n\u001b[1;32m    147\u001b[0m     ) -> None:\n\u001b[1;32m    148\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m         samples = self.make_dataset(\n\u001b[1;32m    151\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mfind_classes\u001b[0;34m(self, directory)\u001b[0m\n\u001b[1;32m    232\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m \u001b[0mof\u001b[0m \u001b[0mall\u001b[0m \u001b[0mclasses\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdictionary\u001b[0m \u001b[0mmapping\u001b[0m \u001b[0meach\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mto\u001b[0m \u001b[0man\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \"\"\"\n\u001b[0;32m--> 234\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mfind_classes\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mSee\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDatasetFolder\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdetails\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \"\"\"\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mentry\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Couldn't find any class folder in {directory}.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/hymenoptera_data/train'"
          ]
        }
      ],
      "source": [
        "# Data augmentation and normalization for training\n",
        "# Just normalization for validation\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        #随机裁剪图像，并将其调整为 224x224 大小，模拟不同场景下的物体尺度变化\n",
        "        transforms.RandomResizedCrop(224),\n",
        "        transforms.RandomHorizontalFlip(), #随机水平翻转图像，增强数据多样性\n",
        "        transforms.ToTensor(),       # (C, H, W)\n",
        "        #使用给定均值 [0.485, 0.456, 0.406]\n",
        "        #和标准差 [0.229, 0.224, 0.225] 对图像归一化\n",
        "        #这些值是 ImageNet 数据集的通用标准\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize(256),    #将图像的最短边调整为 256 像素\n",
        "        transforms.CenterCrop(224),  #从中心裁剪, 并放缩为224x224 的图像\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}\n",
        "\n",
        "data_dir = '/content/drive/MyDrive/hymenoptera_data'\n",
        "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
        "                                          data_transforms[x]) #预处理\n",
        "                  for x in ['train', 'val']}\n",
        "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4,\n",
        "                                             shuffle=True, num_workers=4)\n",
        "              for x in ['train', 'val']} #num_workers=4：启用4个子进程来并行加载数据\n",
        "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
        "class_names = image_datasets['train'].classes\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "-rv1pCNp8_fJ",
        "outputId": "c1aa1dca-0b8a-41cc-e91e-36ceadc463f7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8AtZX9Tb85Pa"
      },
      "source": [
        "Visualize a few images\n",
        "======================\n",
        "\n",
        "Let\\'s visualize a few training images so as to understand the data\n",
        "augmentations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XqxuKz7c85Pb"
      },
      "outputs": [],
      "source": [
        "def imshow(inp, title=None):\n",
        "    \"\"\"Display image for Tensor.\"\"\"\n",
        "    inp = inp.numpy().transpose((1, 2, 0)) #将输入张量转换为NumPy数组，（C，H，W）→（H，W，C）\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "    inp = std * inp + mean #把归一化后的图像还原\n",
        "    inp = np.clip(inp, 0, 1) #将图像像素值限制在[0,1]范围内，确保显示图像时颜色值有效\n",
        "    plt.imshow(inp)\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "    plt.pause(0.001)  # pause a bit so that plots are updated， 适用于交互式模式\n",
        "\n",
        "\n",
        "# Get a batch of training data\n",
        "inputs, classes = next(iter(dataloaders['train']))\n",
        "'''\n",
        "inputs：一个批次的图像张量，形状为 (batch_size, 3, H, W)。\n",
        "classes：一个批次的类别标签，形状为 (batch_size,)\n",
        "'''\n",
        "\n",
        "# Make a grid from batch\n",
        "out = torchvision.utils.make_grid(inputs) #将一个批次的图像张量排列成一个网格，默认网格排列为行优先\n",
        "# 输出一个包含多个图像的网格张量，形状为(3, H, W)\n",
        "\n",
        "imshow(out, title=[class_names[x] for x in classes])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gtXBJQh185Pb"
      },
      "source": [
        "Training the model\n",
        "==================\n",
        "\n",
        "Now, let\\'s write a general function to train a model. Here, we will\n",
        "illustrate:\n",
        "\n",
        "-   Scheduling the learning rate\n",
        "-   Saving the best model\n",
        "\n",
        "In the following, parameter `scheduler` is an LR scheduler object from\n",
        "`torch.optim.lr_scheduler`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6vgtwnnD85Pc"
      },
      "outputs": [],
      "source": [
        "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
        "  #criterion：损失函数，用于计算预测与真实值之间的误差。\n",
        "    since = time.time()\n",
        "\n",
        "    # Create a temporary directory to save training checkpoints\n",
        "    with TemporaryDirectory() as tempdir:\n",
        "        best_model_params_path = os.path.join(tempdir, 'best_model_params.pt')\n",
        "\n",
        "        torch.save(model.state_dict(), best_model_params_path)\n",
        "        # state_dict()：键是模型中每个参数的名称（如层名称），值是参数的值\n",
        "        '''\n",
        "        1. 保存模型的状态字典：将model.state_dict() 保存到指定路径best_model_params_path。\n",
        "        状态字典是一个 OrderedDict，包含了模型的所有可学习参数（如权重和偏置）及其当前值。\n",
        "        2. 只保存参数，不包括模型结构，这种方式通常用来在训练期间或训练完成后保存模型的权重。\n",
        "        '''\n",
        "        best_acc = 0.0 #用于跟踪验证集的最高精度。\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            print(f'Epoch {epoch}/{num_epochs - 1}') #打印进度\n",
        "            print('-' * 10)\n",
        "\n",
        "            # Each epoch has a training and validation phase\n",
        "            for phase in ['train', 'val']:\n",
        "                if phase == 'train':\n",
        "                    model.train()  # Set model to training mode\n",
        "                else:\n",
        "                    model.eval()   # Set model to evaluate mode\n",
        "\n",
        "                #初始化变量，累积当前epoch损失（running_loss）\n",
        "                #和预测正确的样本数量（running_corrects）\n",
        "                running_loss = 0.0\n",
        "                running_corrects = 0\n",
        "\n",
        "                # Iterate over data.\n",
        "                for inputs, labels in dataloaders[phase]:\n",
        "                    #获取输入和标签，传递到设备\n",
        "                    inputs = inputs.to(device)\n",
        "                    labels = labels.to(device)\n",
        "\n",
        "                    # zero the parameter gradients\n",
        "                    optimizer.zero_grad()\n",
        "\n",
        "                    # forward\n",
        "                    # track history if only in train\n",
        "                    with torch.set_grad_enabled(phase == 'train'):\n",
        "                        outputs = model(inputs)\n",
        "                        #获取模型输出中每个样本的预测类别索引\n",
        "                        _, preds = torch.max(outputs, 1)\n",
        "                        loss = criterion(outputs, labels)\n",
        "\n",
        "                        # backward + optimize only if in training phase\n",
        "                        if phase == 'train':\n",
        "                            loss.backward()\n",
        "                            optimizer.step()\n",
        "\n",
        "                    # statistics\n",
        "                    running_loss += loss.item() * inputs.size(0)\n",
        "                    running_corrects += torch.sum(preds == labels.data)\n",
        "                if phase == 'train':\n",
        "                    scheduler.step()\n",
        "\n",
        "                #计算当前阶段的平均损失和准确率\n",
        "                epoch_loss = running_loss / dataset_sizes[phase]\n",
        "                epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "\n",
        "                print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
        "\n",
        "                # deep copy the model\n",
        "                if phase == 'val' and epoch_acc > best_acc:\n",
        "                    best_acc = epoch_acc\n",
        "                    torch.save(model.state_dict(), best_model_params_path)\n",
        "                    #仅在验证阶段，如果当前精度高于最佳精度，保存模型参数\n",
        "\n",
        "            print()\n",
        "\n",
        "        time_elapsed = time.time() - since\n",
        "        print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
        "        print(f'Best val Acc: {best_acc:4f}') #打印最佳验证精度\n",
        "\n",
        "        # load best model weights\n",
        "        model.load_state_dict(torch.load(best_model_params_path, weights_only=True))\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Cisdcc285Pc"
      },
      "source": [
        "Visualizing the model predictions\n",
        "=================================\n",
        "\n",
        "Generic function to display predictions for a few images\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YDLkSnc585Pd"
      },
      "outputs": [],
      "source": [
        "def visualize_model(model, num_images=6):\n",
        "    was_training = model.training #检查模型当前是否处于训练模式\n",
        "    model.eval()\n",
        "    images_so_far = 0 #记录已经可视化的图像数量\n",
        "    fig = plt.figure()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (inputs, labels) in enumerate(dataloaders['val']):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1) #预测类别索引\n",
        "\n",
        "            for j in range(inputs.size()[0]): #inputs.size()[0]：获取当前批次的图片数量\n",
        "                images_so_far += 1\n",
        "                ax = plt.subplot(num_images//2, 2, images_so_far)\n",
        "                ax.axis('off') #隐藏坐标轴\n",
        "                ax.set_title(f'predicted: {class_names[preds[j]]}')\n",
        "                imshow(inputs.cpu().data[j]) #从 GPU 转回 CPU，且转换为 NumPy\n",
        "\n",
        "                if images_so_far == num_images:\n",
        "                    model.train(mode=was_training)\n",
        "                    return\n",
        "        model.train(mode=was_training)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRR137lT85Pd"
      },
      "source": [
        "Finetuning the ConvNet\n",
        "======================\n",
        "\n",
        "Load a pretrained model and reset final fully connected layer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AiQtaYk085Pd"
      },
      "outputs": [],
      "source": [
        "model_ft = models.resnet18(weights='IMAGENET1K_V1') #加载ResNet-18结构。使用在ImageNet数据集上预训练的权重。\n",
        "num_ftrs = model_ft.fc.in_features #提取最后一层的输入特征数（通常为 512）\n",
        "# Here the size of each output sample is set to 2.\n",
        "# Alternatively, it can be generalized to ``nn.Linear(num_ftrs, len(class_names))``.\n",
        "model_ft.fc = nn.Linear(num_ftrs, 2) #用于分类的全连接层\n",
        "\n",
        "model_ft = model_ft.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss() #用于分类任务的交叉熵损失函数\n",
        "#输入是未归一化的 logits（预测值）。输出是类别索引的标签\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Decay LR by a factor of 0.1 every 7 epochs （每隔step_size个epoch，将学习率乘以gamma）\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RKoPYRB85Pd"
      },
      "source": [
        "Train and evaluate\n",
        "==================\n",
        "\n",
        "It should take around 15-25 min on CPU. On GPU though, it takes less\n",
        "than a minute.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s-Dh7sdR85Pd"
      },
      "outputs": [],
      "source": [
        "model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n",
        "                       num_epochs=25)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UEN3gs5y85Pe"
      },
      "outputs": [],
      "source": [
        "visualize_model(model_ft) #随机选择若干张图片，显示其图像、预测类别以及对应的标签"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpDeBQvB85Pe"
      },
      "source": [
        "ConvNet as fixed feature extractor\n",
        "==================================\n",
        "\n",
        "Here, we need to freeze all the network except the final layer. We need\n",
        "to set `requires_grad = False` to freeze the parameters so that the\n",
        "gradients are not computed in `backward()`.\n",
        "\n",
        "You can read more about this in the documentation\n",
        "[here](https://pytorch.org/docs/notes/autograd.html#excluding-subgraphs-from-backward).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kZjuJMXu85Pe"
      },
      "outputs": [],
      "source": [
        "model_conv = torchvision.models.resnet18(weights='IMAGENET1K_V1')\n",
        "for param in model_conv.parameters():\n",
        "    param.requires_grad = False #冻结模型所有参数的梯度更新，减少计算量并保留预训练权重\n",
        "\n",
        "# Parameters of newly constructed modules have requires_grad=True by default\n",
        "num_ftrs = model_conv.fc.in_features\n",
        "model_conv.fc = nn.Linear(num_ftrs, 2)\n",
        "\n",
        "model_conv = model_conv.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Observe that only parameters of final layer are being optimized as\n",
        "# opposed to before.\n",
        "optimizer_conv = optim.SGD(model_conv.fc.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Decay LR by a factor of 0.1 every 7 epochs\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=7, gamma=0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y78wgyDz85Pe"
      },
      "source": [
        "Train and evaluate\n",
        "==================\n",
        "\n",
        "On CPU this will take about half the time compared to previous scenario.\n",
        "This is expected as gradients don\\'t need to be computed for most of the\n",
        "network. However, forward does need to be computed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YKgIw3cw85Pe"
      },
      "outputs": [],
      "source": [
        "model_conv = train_model(model_conv, criterion, optimizer_conv,\n",
        "                         exp_lr_scheduler, num_epochs=25)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fxj3nTak85Pe"
      },
      "outputs": [],
      "source": [
        "visualize_model(model_conv)\n",
        "\n",
        "plt.ioff() #关闭Matplotlib的交互模式\n",
        "plt.show() #在非交互模式下，plt.show() 确保所有绘图内容都呈现出来"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "010ukfWQ85Pe"
      },
      "source": [
        "Inference on custom images\n",
        "==========================\n",
        "\n",
        "Use the trained model to make predictions on custom images and visualize\n",
        "the predicted class labels along with the images.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PSO6HPXx85Pe"
      },
      "outputs": [],
      "source": [
        "def visualize_model_predictions(model,img_path):\n",
        "    was_training = model.training #记录模型的当前状态\n",
        "    model.eval()\n",
        "\n",
        "    img = Image.open(img_path) #加载指定路径的图像\n",
        "    img = data_transforms['val'](img) #应用数据增强，将图像转换为适合模型输入的格式。\n",
        "    img = img.unsqueeze(0) #在最前面添加一个维度，形状从[C, H, W]变为[1, C, H, W]，模拟批量输入。\n",
        "    img = img.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(img)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "\n",
        "        ax = plt.subplot(2,2,1)\n",
        "        ax.axis('off')\n",
        "        ax.set_title(f'Predicted: {class_names[preds[0]]}')\n",
        "        imshow(img.cpu().data[0])\n",
        "\n",
        "        model.train(mode=was_training)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lRwucOY385Pe"
      },
      "outputs": [],
      "source": [
        "visualize_model_predictions(\n",
        "    model_conv, #利用模型进行推理，预测类别，并可视化图像及其预测结果\n",
        "    img_path='data/hymenoptera_data/val/bees/72100438_73de9f17af.jpg'\n",
        ")\n",
        "\n",
        "plt.ioff()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-N4d6ts85Pe"
      },
      "source": [
        "Further Learning\n",
        "================\n",
        "\n",
        "If you would like to learn more about the applications of transfer\n",
        "learning, checkout our [Quantized Transfer Learning for Computer Vision\n",
        "Tutorial](https://pytorch.org/tutorials/intermediate/quantized_transfer_learning_tutorial.html).\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}